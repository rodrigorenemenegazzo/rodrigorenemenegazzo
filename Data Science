The data science pipeline is made up of several stages which include:

Obtaining data
This is where data from both internal, external, and third-party sources is collected and transformed into a usable format (XML, JSON, .csv, etc.). 

Cleaning data
This is the most time-consuming step of the process. Data may contain anomalies such as duplicate parameters, missing values, or irrelevant information that must be cleaned prior to creating a data visualization. 

Cleansing data can be divided into two categories:

Examining data to identify errors, missing values, or corrupt records.
Cleaning data, which involves filling holes, correcting errors, removing duplicates, and throwing away irrelevant records or information.
You may need to recruit a domain expert during this stage to help understand data and the impact of specific features or values.

Exploring and modeling data
After data has been thoroughly cleaned, it can then be used to find patterns and values using data visualization tools and charts. This is where machine learning tools come into play. Using algorithms such as classification accuracy, confusion matrix, logarithmic loss, and others, you can find patterns and apply specific rules to data or data models. These rules can then be tested on sample data to determine how performance, revenue, or growth would be affected.

Interpreting data
The objective of this step is to first identify insights and correlate them to your data findings. You can then communicate your findings to business leaders or fellow colleagues using charts, dashboards, or reports.

Revising data
As business requirements change or more data becomes available, it’s important to periodically revisit your model and make revisions as needed. 

 source: https://www.domo.com/glossary/what-is-the-data-science-pipeline#:~:text=The%20data%20science%20pipeline%20refers,insights%20based%20on%20real%20data.
