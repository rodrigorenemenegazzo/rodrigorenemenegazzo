{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT5R5tVKr7cg+Egwl8S4Gw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigorenemenegazzo/rodrigorenemenegazzo/blob/main/Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Mining**"
      ],
      "metadata": {
        "id": "aAgXyF4mIWcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Text mining (also known as text analysis), is the process of transforming unstructured text into structured data for easy analysis. Text mining uses natural language processing (NLP), allowing machines to understand the human language and process it automatically.\" [Source](https://monkeylearn.com/text-mining/#:~:text=Text%20mining%20(also%20known%20as,language%20and%20process%20it%20automatically.)"
      ],
      "metadata": {
        "id": "17B0LFeoIC8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYyPrvdcH--2"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue May  3 19:47:22 2022\n",
        "\n",
        "@author: Rodrigo\n",
        "\"\"\"\n",
        "# enunciado exercicio\n",
        "# https://www.tads.ufpr.br/pluginfile.php/18895/mod_resource/content/0/04_text_mining.pdf\n",
        "\n",
        "import pandas as pd     \n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "#Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "\n",
        "#Import countVectorizer and create\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "#Random Forest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load the book\n",
        "df_train = pd.read_csv(\"Harrypotter_livro.txt\", \n",
        "                              header=0, #contagem de linhas \n",
        "                              delimiter=\"\\t\",  \n",
        "                              quoting=3)\n",
        "\n",
        "\n",
        "print(df_train.shape)\n",
        "df_train.info()\n",
        "\n",
        "print(df_train.columns.values)\n",
        "print(df_train)\n",
        "\n",
        "#PreProcessing data for all of the training data\n",
        "# https://www.kaggle.com/code/blurredmachine/bag-of-words-meets-random-forest/notebook\n",
        "\n",
        "\n",
        "training_data_size = df_train['DADOS DE COPYRIGHT'].size\n",
        "print(training_data_size)\n",
        "\n",
        "#Funçao para limpar os dados e extrair apenas as palavras de interesse\n",
        "def clean_text_data(data_point, data_size):\n",
        "    #PreProcessing remove tags etc html code\n",
        "    review_soup = BeautifulSoup(data_point)\n",
        "    #Pega as palavras\n",
        "    review_text = review_soup.get_text()\n",
        "    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
        "    #tranforma todas as palavras em letra minuscula\n",
        "    review_lower_case = review_letters_only.lower()  \n",
        "    review_words = review_lower_case.split() \n",
        "    #Mostra stopwords da lingua portuguesa\n",
        "    stop_words = stopwords.words(\"portuguese\")\n",
        "    meaningful_words = [x for x in review_words if x not in stop_words]\n",
        "    \n",
        "    if( (i)%2000 == 0 ):\n",
        "        print(\"Cleaned %d of %d data (%d %%).\" % ( i, data_size, ((i)/data_size)*100))\n",
        "        \n",
        "    return( \" \".join(meaningful_words))\n",
        "\n",
        "#Cabeçalho dos dados de treinamento\n",
        "df_train.head()\n",
        "\n",
        "#Limpando os dados, removendo stop words em portugues\n",
        "for i in range(training_data_size):\n",
        "    df_train['DADOS DE COPYRIGHT'][i] = clean_text_data(df_train['DADOS DE COPYRIGHT'][i], training_data_size)\n",
        "print(\"Cleaning completed!\")\n",
        "print(df_train)\n",
        "\n",
        "#Representação vetorial das setenças/palavras \n",
        "# https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/\n",
        "vectorizer.fit(df_train['DADOS DE COPYRIGHT']) \n",
        "# Printing the identified Unique words along with their indices\n",
        "#The numbers do not represent the count of the words but the position of the words in the matrix\n",
        "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        "\n",
        "#Count words and others things\n",
        "# https://investigate.ai/text-analysis/counting-words-with-scikit-learns-countvectorizer/\n",
        "count_words = vectorizer.fit_transform(df_train['DADOS DE COPYRIGHT'])\n",
        "print(\"\\n The numbers in bracket are the index of the value in the matrix (row, column) and 1 is the value(The number of times a term appeared in the document represented by the row of the matrix) \\n\", \n",
        "      \"\\n Contagem das palavras: \\n\", count_words)\n",
        "\n",
        "#Nome de todas as palavras armazenadas\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "# Encode the Document to be trained for some AI algorithm \n",
        "vector = vectorizer.transform(df_train['DADOS DE COPYRIGHT'])\n",
        "# Summarizing the Encoded Texts\n",
        "print(\"Encoded Document is:\")\n",
        "vector_array = vector.toarray()\n",
        "print(vector_array)\n",
        "print(vector_array.shape)\n",
        "\n",
        "# Regras de associação, clusterização com k-means\n",
        "# http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n",
        "#Clustering\n",
        "# https://stackoverflow.com/questions/54939424/plotting-vectorized-text-documents-in-matplotlib\n",
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(\n",
        "    n_clusters=3, \n",
        "    init='k-means++', \n",
        "    max_iter=500)\n",
        "km.fit(vector_array)\n",
        "\n",
        "#Dimensionality Reduction with PCA then plot the two most important principle components (the first two).\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# First: for every document we get its corresponding cluster\n",
        "clusters = km.predict(vector_array)\n",
        "\n",
        "# We train the PCA on the dense version of the tf-idf. \n",
        "pca = PCA(n_components=2)\n",
        "two_dim = pca.fit_transform(vector_array.todense())\n",
        "\n",
        "scatter_x = two_dim[:, 0] # first principle component\n",
        "scatter_y = two_dim[:, 1] # second principle component\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----------------------\n",
        "#Complex way to count words\n",
        "matrix = vectorizer.fit_transform(df_train['DADOS DE COPYRIGHT'])\n",
        "counts = pd.DataFrame(matrix.toarray(),\n",
        "                      columns=vectorizer.get_feature_names())\n",
        "\n",
        "# Show us the top 10 most common words\n",
        "counts.T.sort_values(by=0, ascending=False).head(10)\n",
        "\n",
        "\n",
        "# (db.scam?)\n",
        "\n",
        "\n",
        "# regras de associação (clusterização com k-means?)\n",
        "# http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n",
        "\n",
        "# contagem de frequência das palavras, manual. \n",
        "\n",
        "\n",
        "#visualize os vetores com a pca.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}